{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(correspsearch-api)=\n",
    "\n",
    "# Data Gathering und Data Wrangling mit Daten der CorrespSearch-API\n",
    "\n",
    ":::{index} correspSearch\n",
    ":name: correspsearch_\n",
    ":::\n",
    "\n",
    "Wenn wir mit Daten arbeiten, ist es eine immer wiederkehrende Aufgabe, diese Daten zunächst zu sammeln und sie für ein Format bzw. für ein Datenmodell aufzubereiten, wie es zur Weiterverarbeitung erforderlich ist. Im Folgenden führen wir diese Schritte exemplarisch durch. Dazu nutzen wir für das {term}`Data Gathering` die {term}`API` des Projekts [correspSearch](https://correspsearch.net/). CorrespSearch bietet zahlreiche {term}`Metadaten` zu einer Fülle an Briefkorrespondenzen. Wir werden uns auf die Briefkorrespondenz einer Person beschränken und werden die Metadaten zu den [Briefen von Alexander von Humboldt](https://correspsearch.net/de/suche.html?s=http://d-nb.info/gnd/118554700) mit Hilfe der API abrufen. Allerdings werden wir für diesen Zweck hier im Juypter Book nicht alle ca. 7.000 bei correspSearch verzeichneten Briefe nutzen, sondern lediglich diejenigen, die im Volltext der [edition humboldt digital](https://edition-humboldt.de/) verfügbar sind - diese Volltexte werden wir im nächsten Kapitel dann auswerten. Wie das Vorgehen ist, um an die Metadaten dieser Briefe heranzukommen, werden wir im Folgenden exemplarisch durchgehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install und Import\n",
    "\n",
    "Für das hier gewählte Vorgehen können wir ein kleines Python-Paket sehr gut nutzen, dass uns bei der Arbeit mit {term}`JSON`-Objekten hilft. Zunächst installieren wir das Paket [`flatten-json`](https://pypi.org/project/flatten-json/) mit Hilfe von {term}`PIP`. Um den Befehl in einer Zelle des Jupyter Notebooks auszuführen, muss dem Befehl pip einfach ein '!' vorangestellt werden. Danach können wir dieses Paket mit den anderen erforderlichen Paketen importieren.\n",
    "\n",
    "Wer nochmal einen Refresher zu JSON benötigt, erfährt dazu alles grundlegende im Jupyter Book 'Python Basics' im Kapitel [Dateien verarbeiten](https://digital-history-berlin.github.io/Python-fuer-Historiker-innen/ch04-dateien-verarbeiten/03-json.html).\n",
    "\n",
    ":::{index} single: Bibliothek ; flatten-json\n",
    ":name: flatten_json_\n",
    ":::\n",
    "\n",
    ":::{index} single: Bibliothek ; os\n",
    ":name: os_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flatten-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API von correspSearch\n",
    "\n",
    "Die Dokumentation zur [correspSearch API v2](https://correspsearch.net/de/api.html) bietet alle wichtigen Informationen, um die Metadaten der in corresSearch vorhandenen Briefeditionen automatisiert abzufragen. Die Abfrage erfolgt mittels URL-Parameter: Mit Hilfe der {term}`Parameter` kann nach Personen/Institutionen, Orten, nach Zeiträumen, Berufen der Personen, nach den Editionen sowie nach Verfügbarkeit gefiltert werden. In der Dokumentation finden sich neben den Erläuterung zu diesen Parametern auch die nötigen Informationen über die unterschiedlichen Rückgabeformate. Für das im Folgenden vorgestellte Vorgehen haben wir uns für das Format {term}`TEI-JSON` entschieden. Weitere Erläuterungen zum Vorgehen bei einer API-Anfragen können im Kapitel \"[Einführung in Weg APIs](intro-api)\" dieses Jupyter Books nachgelesen werden.\n",
    "\n",
    "Bevor wir uns die Zusammensetzung der API-Anfrage zuwenden, schauen wir uns die Daten, die wir zurückerhalten, etwas genauer an. Im `teiHeader` sind allgemeine Metadaten zum Projekt enthalten. Für uns ist das `notesStmt` interessant, denn hier können wir herausfinden, wie viele Treffer unsere Anfrage ergeben hat. In `sourceDesc` finden sich die Informationen zu den Editionen / Publikationen aus denen die Metadaten zu den Briefen entnommen sind. Wir interessieren uns aber vor allem für die Infos in `correspDesc`, den Beschreibungen der angefragten Briefe, die in unter `profileDesc` stehen. Grundsätzlich sind eine Erkundung der Datenmodellierung sowie gute Kenntnisse der Daten von Vorteil und stets sehr hilfreich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"teiHeader\": {\n",
    "        \"fileDesc\": {\n",
    "            \"titleStmt\": {\n",
    "                \"title\": \"correspSearch API 2.0 (BETA)\",\n",
    "                \"editor\": [\n",
    "                    {\n",
    "                        \"#text\": \"correspsearch@bbaw.de\",\n",
    "                        \"email\": \"no-email-provided@correspsearch.net\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"publicationStmt\": {\n",
    "                \"publisher\": [\n",
    "                    {\n",
    "                        \"ref\": {\n",
    "                            \"target\": \"https://www.bbaw.de/\",\n",
    "                            \"#text\": \"Berlin-Brandenburg Academy of Sciences and Humanities\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"availability\": {\n",
    "                    \"licence\": [\n",
    "                        {\n",
    "                            \"target\": \"https://creativecommons.org/licenses/by/4.0/\",\n",
    "                            \"#text\": \"CC-BY 4.0\"\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"idno\": \"https://correspsearch.net/api/v2.0/tei-json.xql?s=http://d-nb.info/gnd/118554700&amp;e=d238c5bd-a978-475e-8784-58ce7ec82010\",\n",
    "                \"date\": {\n",
    "                    \"when\": \"2024-04-18T09:49:17.468+02:00\"\n",
    "                }\n",
    "            },\n",
    "            \"notesStmt\": {\n",
    "                \"note\": \"1-10 of 523 hits\",\n",
    "                \"relatedItem\": {\n",
    "                    \"type\": \"next\",\n",
    "                    \"target\": \"https://correspsearch.net/api/v2.0/tei-json.xql?s=http://d-nb.info/gnd/118554700&amp;e=d238c5bd-a978-475e-8784-58ce7ec82010&amp;x=2\"\n",
    "                }\n",
    "            },\n",
    "            \"sourceDesc\": {\n",
    "                \"bibl\": [\n",
    "                    {\n",
    "                        \"xml:id\": \"d238c5bd-a978-475e-8784-58ce7ec82010\",\n",
    "                        \"type\": \"online\",\n",
    "                        \"#text\": \"edition humboldt digital, hg. v. Ottmar Ette. Berlin-Brandenburgische Akademie der Wissenschaften, Berlin 2017–2023.\",\n",
    "                        \"ref\": {\n",
    "                            \"target\": \"https://edition-humboldt.de\",\n",
    "                            \"#text\": \"https://edition-humboldt.de\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "        \"profileDesc\": {\n",
    "            \"correspDesc\": [\n",
    "                {\n",
    "                    \"ref\": \"https://edition-humboldt.de/H0002656\",\n",
    "                    \"source\": \"#d238c5bd-a978-475e-8784-58ce7ec82010\",\n",
    "                    \"correspAction\": [\n",
    "                        {\n",
    "                            \"type\": \"sent\",\n",
    "                            \"persName\": [\n",
    "                                {\n",
    "                                    \"ref\": \"http://d-nb.info/gnd/118554700\",\n",
    "                                    \"#text\": \"Alexander von Humboldt\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"placeName\": [\n",
    "                                {\n",
    "                                    \"ref\": \"http://sws.geonames.org/2911298\",\n",
    "                                    \"#text\": \"Hamburg\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"date\": [\n",
    "                                {\n",
    "                                    \"from\": \"1791-01-28\",\n",
    "                                    \"to\": \"1791-02-20\"\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"received\",\n",
    "                            \"persName\": [\n",
    "                                {\n",
    "                                    \"ref\": \"http://d-nb.info/gnd/118805193\",\n",
    "                                    \"#text\": \"Samuel Thomas von Soemmerring\"\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstruktion der API-Anfrage\n",
    "\n",
    "Nachdem wir uns gerade die Metadaten zum ersten Brief angesehen haben, betrachten wir nun die Konstruktion der API-Anfrage genauer. \n",
    "\n",
    "* Zunächst setzen wir die Variable `api_base_url` auf das Rückgabeformat `TEI-JSON`.\n",
    "\n",
    "* Dann nutzen wir für die Variable `person` die GND-ID für Alexander von Humboldt.\n",
    "\n",
    "* Da wir direkt bei der Anfrage einen ersten Filter einsetzen möchten, mit dem wir nur die Briefe aus der [edition humboldt digital](https://edition-humboldt.de/) zurückerhalten, wählen wir für den Parameter `e=` die ID für diese Edition aus und weisen diese der Variable `edition` zu.\n",
    "\n",
    "Wo finden wir aber die ID, die für die Edition bzw. den Publikationsort der Briefe, die uns interessieren, steht? Bei einer allgemeine API-Anfrage nach Briefen von Alexander von Humboldt müssen wir uns die JSON-Datei genauer anschauen. Hier finden wir bei den entsprechenden Briefen und `sourceDesc` die Informationen zu dem entsprechenden Datengeber. Unter `sourceDesc`, `bibl` und dann `xml:id` finden wir einen langen String, der die ID für die entsprechende Edition darstellt. Diesen String können wir bei einer neuen Anfrage an die API nutzen, um nach Treffern, die nur von dieser Edition stammen, zu filtern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEI-JSON\n",
    "api_base_url = 'https://correspsearch.net/api/v2.0/tei-json.xql?'\n",
    "\n",
    "# GND-ID für Alexander von Humboldt\n",
    "person = 'http://d-nb.info/gnd/118554700'\n",
    "\n",
    "# Parameter für Edition / Publikation\n",
    "edition = 'd238c5bd-a978-475e-8784-58ce7ec82010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließlich setzen wir die einzelnen Bausteine zur API-Anfrage zusammen. Angaben zur Filterung werden jeweils mit dem `&` angefügt. Mit `print` können wir den Anfrage-String ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_query = f'{api_base_url}s={person}&e={edition}'\n",
    "print(api_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereiten der API-Anfrage\n",
    "\n",
    "Als nächsten nutzen wir das Python-Modul `requests`, um die Anfrage einmalig zu stellen. Wir können dann nachsehen, wieviele Treffer die Anfrage ergeben hat, um für die Konstruktion der eigentlichen Anfrage eine wichtige Information zu erhalten. Denn: Pro Anfrage werden jeweils die Informationen für 100 Treffer zurückgegeben, wir müssen also eine Schleife nutzen -- und hierfür benötigen wir die Anzahl, wie oft diese Schleife durchlaufen werden soll. Dazu greifen wir über `['teiHeader']['fileDesc']['notesStmt']['note']` auf den String zu, der die Angaben zu den *hits* enthält. Den String splitten wir auf, nutzen das vorletzte Elemente der durch den `split`-Befehl entstehenden Liste, wandeln dieses in ein integer um und teilen diese Zahl mit der *floor division* durch 100 auf die abgerundete Ganzzahl, um dann noch 1 hinzu zu addieren.\n",
    "\n",
    "Diesen so berechnten Wert nutzen wir wiederum für den URL-Parameter `x=`, der für die Paginierung steht. Auf diese Weise erhalten wir die Treffer in Blöcken zu jeweils 100 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(api_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['teiHeader']['fileDesc']['notesStmt']['note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_pages = int(response.json()['teiHeader']['fileDesc']['notesStmt']['note'].split()[-2]) // 10 + 1\n",
    "print(record_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der nächsten Code-Zelle legen wir einen `data`-Ordner an, um dort die JSON-Dateien zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print('Ordner existiert bereits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchführung der API-Anfrage\n",
    "\n",
    "Nun können wir die API-Anfrage starten. Dazu durchlaufen wir die Schleife entsprechend der Anzahl der Treffer in hunderter Blöcken. Nach jedem Durchlauf wird der Paginierungs-Parameter `x=` um eins erhöht. Wir geben dann die Angabe über den Fortgang des Durchlaufs mit dem `print`-Befehl aus. Danach konstruieren wir einen Dateinamen; dazu nutzen wir wiederum die Laufvariable der Schleife. Die erhaltenen Daten speichern wir im JSON-Format ab. Zuletzt warten wir 5 Sekunden, um die API mit den Anfragen nicht zu stark zu belasten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_nr in range(1, record_pages + 1):\n",
    "\n",
    "    response = requests.get(f'{api_query}&x={page_nr}')\n",
    "\n",
    "    print(response.json()['teiHeader']['fileDesc']['notesStmt']['note'])\n",
    "\n",
    "    with open(f'data/{str(page_nr).zfill(4)}_AvH_edi_hum_dig.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(response.json(), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns den Inhalt des `data`-Ordners kurz an, ob alles passt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data')\n",
    "print(len(files))\n",
    "print(sorted(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Von JSON zum Pandas Dataframe\n",
    "\n",
    "Das nächste Ziel ist es, die Daten aus den JSON-Dateien in einen pandas Dataframe zu bringen. Die in der Variable `files` enthaltenen und sortierten Dateien durchlaufen wir mit einer for-Schleife: Wir öffnen die Datei, lesen die JSON ein und nutzen nun die Methode `flatten` aus dem Paket `flatten-json`, um mit Hilfe einer List Comprehension die verschachtelte Stuktur der Dictionaries 'einzuebnen'. Mit `extend()` fügen wir die jeweiligen Listen einer neu erstellten Liste hinzu. Diese Liste, die alle JSON-Objekte enthält, können wir nun in einen Dataframe überführen. Wir nutzen `flatten` allerdings lediglich für die Dictionaries, die in `correspDesc` enthalten sind. Daher setzen wir die Indexierung über `['teiHeader']['profileDesc']['correspDesc']` in den JSON-Objekten auf diesen für uns relevanten Abschnitt der Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json_obj_flattened = []\n",
    "\n",
    "for file in sorted(files):\n",
    "\n",
    "    if file.endswith('.json'):\n",
    "\n",
    "        with open(f'data/{file}', 'r', encoding='utf8') as f:\n",
    "\n",
    "            json_obj = json.load(f)\n",
    "\n",
    "            dic_flattened = [ flatten(dic) for dic in json_obj['teiHeader']['profileDesc']['correspDesc'] ]\n",
    "\n",
    "            all_json_obj_flattened.extend(dic_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_json_obj_flattened)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbereiten des Dataframes\n",
    "\n",
    "Wir erhalten nun einen Dataframe mit 523 Zeilen und 28 Spalten. Mit `df.isna().sum()` können wir anzeigen, wie viele der Werte leer geblieben, also {term}`NANs` (not a number) sind. In den nächsten Schritten wollen wir diesen Dataframe weiter aufbereiten, denn wir wollen für die Zwecke dieses JupyterBooks einen etwas kleineren Datensatz nutzen. Um den Umfang zu reduzieren, werden wir nur die Datensätze der Briefe nutzen, deren Datierung genau angegeben ist. Diese Info finden wir in der Spalte `correspAction_0_date_0_when`, in der auch noch 227 Werte fehlen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden wir mit `dropna()` verschiedene Spalten und Zeilen löschen, in denen keine Werte enthalten sind. Wir nutzen das Argument `subset=`, um Spalten gezielt auszuwählen und wir verwenden auch das Argument `inplace=True`, um die Änderungen direkt an dem Dataframe zu vollziehen.\n",
    "\n",
    ":::{index} single: pandas ; dropna()\n",
    ":name: dropna_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['correspAction_0_date_0_when'], inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Zelle erstellen wir einen Dataframe, der die Informationen zu Ortsangaben zwischenspeichert. In der Zelle darauf können wir nun alle Spalten, die leere Werte enthalten, löschen. Wir erhalten nun einen Dataframe mit 296 Zeilen und nur noch 9 Spalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_place = df.loc[:, ['correspAction_0_placeName_0_ref', 'correspAction_0_placeName_0_#text']]\n",
    "df_place.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ferner löschen wir noch Spalten, die die Info `sent` bzw. `receive` enthalten. Durch eine spätere Umbenennung der Spalten können wir diese Infos über Sender und Empfänger jedoch in einer anderen Spalten mit unterbringen. Nun haben wir nur noch 7 Spalten und alle 296 Zeilen enthalten einen Wert in den einzelnen Zellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['correspAction_0_type', 'correspAction_1_type'], axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt fügen wir die zwischengespeicherten Infos zu den Ortsangaben dem Dataframe mit `join()` wieder hinzu: Das Ergebnis ist ein Dataframe mit 296 Zeillen und 9 Spalten.\n",
    "\n",
    ":::{index} single: pandas ; join()\n",
    ":name: join_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df_place)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den letzten Schritten benennen wir die Spaltennamen um und reseten den Index der Zeilen, sodass wir eine saubere Durchnummerierung der Zeilen von 0 bis 295 erhalten. Zum Schluss speichern wir den Dataframe als csv-Datei ab.\n",
    "\n",
    ":::{index} single: pandas ; reset_index()\n",
    ":name: reset_index_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.columns = ['reference', 'edition_id', 'sender_id', 'sender', 'receiver_id', 'receiver', 'date', 'place_id', 'place' ]\n",
    "\n",
    "# reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Schluss speichern wir den Dataframe als CSV-Datei ab.\n",
    "\n",
    ":::{index} single: pandas ; to_csv()\n",
    ":name: to_csv_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df.to_csv('data/AvH-letters-with-date.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_2_expand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
