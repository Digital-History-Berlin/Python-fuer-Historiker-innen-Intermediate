{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von TEI-XML zu Volltext und Tokenisierung\n",
    "\n",
    "In diesem Abschnitt werden wir die im vorherigen Unterkapitel gesammelten Daten nutzen, um die Volltexte der Briefe in unseren Dataframe zu laden. Hier nochmal der Hinweis, dass wir eine Auswahl getroffen haben: Wir werden nur die Briefe herunterladen, die in der [edition humboldt digital](https://edition-humboldt.de/) enthalten sind und die eine genaue Datierung besitzen. Da wir über die Anfrage die Briefe im TEI-XML Format erhalten, wir aber nur mit den Volltexten arbeiten wollen, ergibt sich daraus für uns das Ziel, die xml-tags zu entfernen. Zunächst aber laden wir die benötigten Python-Pakete und die Daten, die wir in einer csv-Datei gespeichert haben. \n",
    "\n",
    ":::{index} single: Bibliothek ; string\n",
    ":name: string_\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import und Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Einlesen der Daten wird mit dem Argument `parse_date=` die Spalte mit Datumsangaben in ein Datetime-Objekt umgewandelt. Dies ermöglicht weitere Optionen für das Arbeiten mit Zeitangaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/AvH-letters-with-date.csv', parse_dates=['date'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren von Funktionen\n",
    "\n",
    "Pandas bietet den Vorteil, einzelne Spalten eines Dataframes, also die Series, zu bearbeiten. Hierzu können die nötigen Funktionen entsprechend definiert und mit `apply()` angewandt werden. `apply()` ist wesentlich performanter als for-Schleifen, die hier ja eigentlich auch in Frage kommen könnten. \n",
    "\n",
    "Die folgenden Funktionen nutzen wir später, um die TEI-XML Dateien herunterzuladen; hierzu nutzen wir die Spalte `reference`, denn hier sind die URLs der Dateien zu finden. Danach ziehen wir den reinen Text aus dem XML-Format heraus, um bei dem erhaltenen Text die Zeichensetzung zu entfernen und um diesen in {term}`Token` zu zerlegen. Die Doc-Strings zu den Funktionen geben zusätzlich kurze Infos dazu, was die jeweilige Funktion ausführt.\n",
    "\n",
    ":::{index} single: pandas ; apply()\n",
    ":name: apply_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_file(reference):\n",
    "    '''\n",
    "    Function makes URL-request.\n",
    "    Concatenates string and uses requests to get file.\n",
    "\n",
    "    Return: requests-Response-Object    \n",
    "    '''\n",
    "    response = requests.get(reference + '.xml')\n",
    " \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(reference):\n",
    "    '''\n",
    "    Function returns from xml-tags cleaned text.\n",
    "    Calls helperfunction and makes soup to parse xml.\n",
    "    Finds relevant part of xml and strips xml-tags.\n",
    "    \n",
    "    Return: string\n",
    "    '''\n",
    "    response = get_xml_file(reference)\n",
    "\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    text = soup.find('text') \n",
    "\n",
    "    clean_text = ' '.join(bs(str(text), 'html.parser').stripped_strings)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(text):\n",
    "    '''\n",
    "    Function returns tokens from text.\n",
    "    Removes whitespaces and punctuation.\n",
    "    Normalizes tokens to lowercase.    \n",
    "    \n",
    "    Returns: list    \n",
    "    '''\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    for char in text:\n",
    "\n",
    "        if char in string.punctuation + '—':\n",
    "            text = text.replace(char, '')\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    tokens = [ token.lower() for token in tokens ]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufruf der Funktionen\n",
    "\n",
    "In der nachfolgenden Notebookzelle rufen wir die Funktionen mit `apply()` jeweils auf einer Spalten auf. Die Rückgaben werden direkt neuen Spalten zugewiesen. Wie bei allen Benennungen ist es auch hier empfehlenswert, sprechende Titel für die Spalten zu nutzen. Die Ausführung der Zelle kann etwa 1 Minute dauern, da die Anfragen an einen externen Server gestellt werden. Dann geben wir die Anzahl von Spalten und Zeilen sowie die ersten Zeilen aus, um das Ergebnis zu prüfen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'text'] = df.loc[:, 'reference'].apply(get_text)\n",
    "df.loc[:, 'token'] = df.loc[:, 'text'].apply(get_token)\n",
    "df.loc[:, 'nr_token'] = df.loc[:, 'token'].apply(len)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern der Ergebnisse\n",
    "\n",
    "Abschließend speichern wir die Ergebnisse. Diesmal nutzen wir das csv- und das JSON-Format. Der Vorteil von csv ist, das wir dieses Format auch leicht mit anderen Tabellenkalkulationsprogrammen öffnen können. Allerdings bleiben die Datentypen in den Zellen nicht erhalten - wir können beim Einlesen von csv-Dateien nur mit Strings weiterarbeiten und müssten diese dann konvertieren. JSON bietet hingegen den Vorteil, dass die Datentypen in den jeweiligen Spalten erhalten bleiben: Sind Listen oder Dictionaries in einer Spalten, dann stehen diese Datentypen auch nach dem erneuten Einlesen der JSON-Datei wieder direkt zur Verfügung. Lediglich Spalten, die Datetime-Objekte enthalten, müssten von der unix-Zeit wieder in eine Datetime-Objekt umgewandelt werden. Aber das schauen wir uns im nächsten Abschnitt genauer an. Es hängt also am Ende vom jeweiligen Anwendungszenario ab, welches Dateiformat Sie beim Speichern auswählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/AvH-letters-with-tokens.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('data/AvH-letters-with-tokens.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_2_expand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
