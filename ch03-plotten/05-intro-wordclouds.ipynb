{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordclouds\n",
    "\n",
    "Wordclouds, auch Wortwolken genannt, sind visuelle Darstellungen von Textdaten, bei denen die Häufigkeit der Wörter durch ihre Schriftgröße innerhalb der Wolke dargestellt wird. Je öfter ein Wort im Text vorkommt, desto größer und auffälliger erscheint es in der Wordcloud, wodurch ein schneller visueller Eindruck von den Schlüsselthemen oder -begriffen eines Textes vermittelt wird. Diese Art der Darstellung eignet sich besonders gut für die Analyse von großen Textmengen, um dominante Themen oder Stimmungen zu identifizieren. Wordclouds sind intuitiv verständlich und können effektiv eingesetzt werden, um komplexe Datenmengen auf eine zugängliche und ästhetisch ansprechende Weise zu präsentieren.\n",
    "\n",
    "Das Python-Paket [WordCloud](https://amueller.github.io/word_cloud/) bietet einen einfachen Einstieg, um mit Python Wordclouds zu generieren. Was hier im Bereich besonderer Layouts machbar ist, zeigt die [Galerie](https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Bevor wir loslegen können, müssen die entsprechenden Bibliotheken importiert und die Datengrundlage eingelesen werden. Das Python-Pakete stopword-iso wird uns bei der Entfernung von Stoppwörtern helfen.\n",
    "\n",
    ":::{index} single: Bibliothek ; wordcloud\n",
    ":name: wordcloud_\n",
    ":::\n",
    "\n",
    ":::{index} single: Bibliothek ; stopwordsiso\n",
    ":name: stopwordsiso_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stopwordsiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/AvH-letters-with-tokens.json')\n",
    "df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'date'], unit='ms')\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Einlesen und dem obligatorischen Check der Daten, müssen wir die uns vorliegenden Textdaten in das Format bringen, welches das Python-Paket WordCloud erwartet. In diesem Fall benötigen wir alle darzustellenden Wörter in einem einzigen String. Dazu holen wir aus unserem Dataframe alle Texte aus der Spalte `text` als Liste heraus. Dadurch liegen zwar die einzelnen Elemente dieser Liste als String vor, aber wir benötigen einen einzigen String - daher nutzen wir `' '.join()`, um aus den einzelnen Strings in der Liste einen langen String zu erstellen. Wir normalisieren die Wörter in Kleinschrift mit `lower()`, damit die Worhäufigkeiten besser ermitteln werden. \n",
    "\n",
    "**Wichtig!** Wir haben die Texte nicht noch weitergehend durch Lemmatisierung normalisiert, sodass wir bei der Zählung der Worthäufigkeiten starke Ungenauigkeiten vorliegen. Wie die Wörter von Texten lemmatisiert, also auf ihre Grundform gebracht werden können, demonstrieren wir im nächsten JupyterBook Python advanced. Hier geht es uns vor allem um Vorgehensweise bei der Erstellung von Wordclouds mit Python.\n",
    "\n",
    ":::{index} single: pandas ; to_list()\n",
    ":name: to_list_\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(df.loc[:, 'text'].to_list()).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Würden wir nun direkt unsere Wordcloud generieren, die ja auf der Auszählung von Häufigkeiten beruht, dann würden wir in der Visualisierung vor allem Artikel und Konjuktionen sehen. Das liegt daran, dass diese Wörter, auch als {term}`Stoppwörter` bezeichnet, sehr häufig vorkommen, aber wenig inhaltliche Aussagekraft besitzen. Diese Stoppwörter möchten wir also gerne filtern, was mit WordCloud auch leicht möglich ist. Allerdings müssen wir eine Liste mit Stoppwörtern übergeben. Diese Liste mit Stoppwörtern erstellen wir in der nächsten Codezelle. Die Stoppwörter können wir über das Paket [stopwords-iso](https://github.com/stopwords-iso/stopwords-iso) erhalten, hier können wir auch einsehen, welche Stoppwörter in der jeweiligen Liste enhalten sind. Die Stoppwortlisten können aber auch über andere Quellen bezogen werden bzw. können auch noch selbst angepasst werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.stopwords('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstellen wir das WordCloud-Objekt, das später die Wordcloud generiert. Verschiedene Parameter können hier eingestellt werden: Größe der Visualisierung, Hintergrundfarbe, Farbe der Schrift, die zu nutzenden Stoppwörter sowie die Anzahl der Wörter, die dargestellt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1500, height=1000,\n",
    "            random_state=42,\n",
    "            background_color='black',\n",
    "            colormap='Set1',\n",
    "            stopwords=stop_words,\n",
    "            collocations=False,  # Bigrams werden nicht dargestellt\n",
    "            max_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann generieren wir die Wordcloud und nutzen Matplotlib, um ein wenig das Layout anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate(words) \n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Wordcloud fällt allerdings auf, dass einige Briefe auch in französischer oder englischer Sprache verfasst wurden. Auch diese Sprachen haben zahlreiche Stoppwörter, wie in der Wortwolke zu sehen ist. Folglich sollten wir auch diese Stoppwörter herausnehmen. Dies erreichen wir mit den nachfolgenden Codezellen. Bei den Parametern für die Wordcloud-Generierung haben wir für die Schrift ein anderes Farbset gewählt und zugleich den Hintergrund auf weiß eingestellt. \n",
    "\n",
    "Versuchen Sie, die Parameter selbst nochmal zu verändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_stopwords = stopwords.stopwords(['de', 'en', 'fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1500, height=1000,\n",
    "            random_state=42,\n",
    "            background_color='white',\n",
    "            colormap='Set2',\n",
    "            stopwords=multi_stopwords,\n",
    "            collocations=False,  # Bigrams werden nicht dargestellt\n",
    "            max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate(words) \n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Hilfe einer for-Schleife und boolescher Maskierung können wir auch die Worthäufigkeiten in den Briefen mit Wordclouds für die einzelnen Jahre - hier für die Jahre zwischen 1850 und 1860 -  darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1850,1860))\n",
    "\n",
    "for year in years:\n",
    "    mask = df.loc[:, 'date'].dt.to_period('Y') == str(year)\n",
    "\n",
    "    wc = WordCloud(width=1500, height=1000,\n",
    "            random_state=42,\n",
    "            background_color='black',\n",
    "            colormap='Set2',\n",
    "            stopwords=stop_words,\n",
    "            collocations=False,  # Bigrams werden nicht dargestellt\n",
    "            max_words=25)       \n",
    "\n",
    "\n",
    "    wc.generate(df.loc[mask, 'text'].str.cat(sep=' ').lower()) # str.cat konkateniert alle Wörter zu einem einzigen String\n",
    "                                                                     \n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.title(year)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_2_expand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
