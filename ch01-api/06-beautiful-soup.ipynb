{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML parsen mit Beautiful Soup\n",
    "\n",
    ":::{index} single: Bibliothek ; Beautiful Soup\n",
    ":name: beautiful_soup_\n",
    ":::\n",
    "\n",
    "[`Beautiful Soup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) ist eine Python-Bibliothek mit der Sie die mit `requests` heruntergeladene Seite analysieren ({term}`*to parse*`) können, dies wird ermöglicht durch die gezielte Navigation über die verschiedenen {term}`HTML`-Elemente. Wir werden die jüngste Version `bs4` nutzen. Gegebenenfalls müssen Sie Beautiful Soup in der Version 4.x herunterladen wie im Kapitel \"[Installation von Third-Party-Paketen](third-party-packages)\" beschrieben und in der [Dokumentation der Bibliothek](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup) genau ausgeführt.\n",
    "\n",
    "Wenn Sie die Bibliothek installiert haben, dann importieren wir sie in unsere Programmierumgebung. Diesmal benötigen wir allerdings nicht den gesamten Umfang, sondern lediglich einzelne Bestandteile. Mit dem nachfolgenden Code importieren wir nur das, was wir auch tatsächlich zum Scraping einer Website benötigen. Das spart Rechenleistung! Wir können zugleich die importierten Pakete mit einem Alias versehen, das bietet sich insbesondere bei längeren Namen an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "War die Installation des Pakets erfolgreich, dann sollten Sie ohne Fehlermeldung weiterarbeiten können. Wir können nun die mit `requests` heruntergeladene HTML-Datei in ein `BeautifulSoup`-Objekt transformieren. Standardmäßig wird dazu ein `html-parser` verwendet, der speziell für die Verarbeitung von HTML-Dokumenten entwickelt wurde. Im Paketumfang sind aber auch einige andere Parser enthalten, zum Beispiel einer für XML-Dateien.\n",
    "\n",
    "Das `BeautifulSoup`-Objekt repräsentiert die Website nun als eine verschachtelte Datenstruktur, die wir uns verhältnismäßig übersichtlich mit der Methode `.prettify()` ansehen können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get http with requests\n",
    "response = requests.get('https://www.geschichte.hu-berlin.de/de/bereiche-und-lehrstuehle/digital-history')\n",
    "\n",
    "# create BeautifulSoup-object\n",
    "soup = bs(response.text, \"html.parser\")\n",
    "\n",
    "# print BeautifulSoup-object\n",
    "print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informationen aus der HTML-Struktur auslesen\n",
    "\n",
    "Um den Umgang mit HTML-Dokumente zu üben, nutzen wir als Beispiel die Landing-Page der Professur für Digital History. In den nachfolgenden Codeblöcken sehen Sie einige Beispiele dafür, wie Sie unterschiedliche Elemente innerhalb des Dokuments aufrufen können. \n",
    "\n",
    "Es gibt im Wesentlichen zwei Navigationsmethoden:\n",
    "\n",
    "1.   Angabe von {term}`Tags`, die an das BeautifulSoup-Objekt angehängt werden, z.B. `soup.a `für ein Anker-Element oder `soup.h1` für eine Überschrift erster Ordnung\n",
    "2.   [CSS-Selektoren](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) in Kombination mit der Methode `select()`, z.B. `soup.select(\"div.hu-base-row div > div\")` sucht nach einem `div`-Element, das direkt unter einem `div`-Element angesiedelt ist, das wiederum irgendwo in einem `div`-Element mit der Klasse (`class=\"\"`) `\"hu-base-row\"` eingegliedert ist\n",
    "\n",
    "Wir können uns beispielsweise den Titel einer Website anzeigen lassen, also den Text, der im Browser auf den jeweiligen Tabkarten angezeigt wird. Zu finden ist diese Information in einem `title`-Tag: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title, \"\\n\")\n",
    "print(f\"Der Output \\\"{soup.title.text.strip()}\\\" ist vom Datentyp {type(soup.title.text)}).\\n\")\n",
    "print(f\"Der Output \\\"{soup.title.string}\\\" ist vom Datentyp {type(soup.title.string)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So erhalten wir einmal das vollständige `title`-Element und einmal lediglich die Zeichenkette, die in den `title`-Tags eingefasst ist. Für letzteres gibt es zwei Herangehensweisen:\n",
    "\n",
    "\n",
    "1.   `.text`\n",
    "2.   `.string`\n",
    "\n",
    "Ersteres liefert Ihnen als Datentyp eine einfache Zeichenkette zurück. Wenn ein Tag-Element lediglich Text enthält, dann ist dies ausreichend. Es kann aber auch sein, dass ein HTML-Element noch weitere *children*, also Unterelemente, beinhaltet, wenn Sie in einer solchen Struktur noch tiefergehen möchten, dann bietet es sich an, mit der zweiten Variante weiterzuarbeiten, da Sie hier als Datentypen eine navigierbare Zeichenkette erhalten.\n",
    "\n",
    "**Nice to know:** Einige überflüssige Whitespaces können Sie durch Anhängen von `.strip()` eliminieren.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein HTML-Dokument durchsuchen\n",
    "Tags können verschiedene {term}`Attribute` haben, Anker-Tags beispielsweise das Attribut \"href\", das wiederum häufig eine URL als Wert hat. Wenn wir nicht nur den ersten Link abrufen wollen, sondern alle, dann können wir die Methode `find_all(tagname, attrs, recursive, string, limit, **kwargs)` einsetzen. Der Methode werden quasi Filter übergeben anhand derer das HTML-Dokument analysiert wird. Es werden alle Nachkommen (*descendants*) eines Tags durchsucht und nur diejenigen zurückgegeben, die Ihrem definierten Filter entsprechen.\n",
    "\n",
    "Genutzt werden können dazu einfache Strings oder Listen, um auf Tag-Namen oder Attribute zu referieren, aber auch [Reguläre Ausdrücke](regex). Hier einige Beispiele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve first link with a value for the attribute \"href\"\n",
    "first_link = soup.a[\"href\"]\n",
    "print(\"Erster Link referenziert:\\n\", first_link, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all links with a value for the attribute \"href\", regardless of what the value is\n",
    "all_links = soup.find_all(\"a\", href=True)\n",
    "print(\"Alle Links mit einem Referenzattribut:\\n\", all_links, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all a- and p-tags\n",
    "all_a_and_p = soup.find_all([\"a\", \"p\"])\n",
    "print(\"Alle Anker- und Absatz-Elemente:\\n\", all_a_and_p, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all metadata\n",
    "all_metainfo = soup.find_all(\"meta\")\n",
    "print(\"Alle Metainformationen:\\n\", all_metainfo, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während `find_all()` das komplette Dokument durchsucht und entsprechende Ergebnisse als Liste zurückliefert, kann es freilich auch sein, dass eine ganz bestimmte Information auszulesen ist. Nach einem spezifischen Ergebnis suchen wir mit der Methode `find(tagname, attrs, recursive, string, **kwargs)`.\n",
    "\n",
    "Mit ihr können wir nun beispielsweise den Volltext aus der HTML-Struktur extrahieren. Dazu müssen wir uns anschauen, in welchem HTML-Element sich der Text befindet. Am einfachsten geht dies, wenn wir in den Quellcode schauen. Wie sie im Browser den Quellcode einer Website untersuchen können, können Sie im Exkurs \"[HTML-Basics](html-basics)\" noch einmal nachlesen.\n",
    "\n",
    "Wenn Sie den Text inspiziert haben, werden Sie festgestellt haben, dass er sich aus verschiedenen `p`-Tags zusammensetzt, die *children* eines `div`-Tags mit der ID `id=\"content-core\"` sind. Diese Informationen können wir nun nutzen, um den Text gezielt auszulesen und einer Variablen zuzuweisen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve fulltext\n",
    "content_fulltext = soup.find(\"div\", {\"id\":\"content-core\"}).text.strip() \n",
    "print(content_fulltext)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beachten Sie die Syntax: Wir suchen zunächst nach einem `div`-Element und zwar nach einem `div`-Element mit einer ganz bestimmten ID. Diese ID können wir durch eine Notation definieren, die Sie auch schon von Dictionaries kennen. Wir suchen gewissermaßen nach einem bestimmten Schlüssel mit einem bestimmten Wert. Etwaige HTML-Elemente wie beispielsweise `p`-Tags, die sich in diesem Bereich befinden, können Sie durch das Anhängen von `.text` ausschließen. Überflüssige Whitespaces entfernen wir mit `.strip()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere wichtige Information, die man extrahieren könnte, stellt stets auch das Veröffentlichungsdatum dar. Mit den bisher kennengelernten Inhalten können wir auch dies nach einer Inspektion des Quellcodes herunterladen. Auf der Website befindet sich das Datum in der Fußzeile. Es ist in einem nicht näher spezifizierten `span`-Element eingebettet, das wiederum *child* eines `span`-Elements der Klasse `\"documentModified\"` ist. Diese Informationen sind zu unspezifisch, um das Datum gezielt auszulesen. Was aber direkt ansteuerbar ist, ist der *sibling* mit der Zeichenkette \"zuletzt geändert\". Diese Information können wir uns zu Nutze machen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve date of publication\n",
    "report_issued = soup.find(\"span\", string=\"zuletzt geändert\").next_sibling.text.strip()\n",
    "print(report_issued)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir steuern also zunächst dasjenige `span`-Element an, das einen String des Inhalts \"zuletzt geändert\" enthält. Davon interessiert uns aber nur der Inhalt des direkt darauf folgenden Tags, den wir per `.next_sibling.text` ansteuern. Überflüssige Whitespaces entfernen wir wieder mit `.strip()`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{index} XML\n",
    ":name: xml\n",
    ":::\n",
    "\n",
    "Wenn die Daten einer {term}`API` in {term}`XML` zurückgegeben werden, können Sie mit Beautiful Soup ebenso vorgehen, wie im vorherigen Abschnitt am Beispiel von {term}`HTML` veranschaulicht wurde. XML ist, ähnlich wie HTML, eine Auszeichnungssprache mit der bestimmte Elemente eines Dokuments maschinenlesbar gemacht werden können. Tags und deren Attribute funktionieren in XML analog wie bei HTML. Sie können die XML-Struktur eines Dokuments oftmals auch dazu nutzen, um mit geringem Aufwand Informationen auszulesen. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
