{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(regex-loesung)=\n",
    "# üìù √úbung zu regul√§ren Ausdr√ºcken\n",
    "\n",
    "Nachdem Sie nun die Basics im Umgang mit Regul√§ren Ausdr√ºcken kennengelernt haben, sollen Sie auf Basis des gesamten Tagungsberichts nach einigen Mustern suchen und daf√ºr Regul√§re Ausdr√ºcke formulieren.\n",
    "\n",
    "Extrahieren Sie\n",
    "- alle Zahlen\n",
    "- alle Anmerkungen, wie zum Beispiel \"[1]\"\n",
    "- alle W√∂rter, die mit einem \"A\" oder \"a\" beginnen \n",
    "- alle Bestandteile, die in Anf√ºhrungszeichen gesetzt sind\n",
    "- alle W√∂rter am Ende eines Abschnitts (*Tipp:* Schauen Sie sich in der Dokumentation an, wie der MULTILINE-Modus funktioniert)\n",
    "\n",
    "- Pr√ºfen Sie au√üerdem, ob es hier doppelte Leerzeichen gibt und wenn ja wie oft\n",
    "- Entfernen Sie alle Anmerkungen und √ºberfl√ºssigen Leerzeichen. Weisen Sie den Text einer neuen Variablen zu.\n",
    "\n",
    "Zusatz (optional):\n",
    "- Knobelaufgabe: Extrahieren Sie alle Personennamen (Vor- und Nachname) und Herkunftsorte.\n",
    "\n",
    "Die extrahierten Daten m√ºssen Sie nicht weiterverarbeiten. Es gen√ºgt, wenn Ihr Programm diese mit `print()` ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"Der Arbeitskreis ‚ÄûDigital Humanities Munich‚Äú (dhmuc)[1] widmete seinen ersten Workshop 2016 dem Thema ‚ÄûDigitale Daten in den Geisteswissenschaften. Interdisziplin√§re Perspektiven f√ºr semantische und strukturelle Analysen‚Äú.[2] Insgesamt 14 Vortr√§ge er√∂rterten aktuelle Forschungen und Infrastrukturen im Bereich der maschinellen Textanalyse.\n",
    "Folgende Institutionen zeichneten f√ºr die Organisation des Workshops verantwortlich: dhmuc ‚Äì Arbeitskreis ‚ÄúDigital Humanities Munich‚Äù, das Institut f√ºr Computerlinguistik Universit√§t Z√ºrich [3], das Historische Seminar der Ludwig-Maximilians-Universit√§t M√ºnchen [4], die IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universit√§t M√ºnchen [5] sowie die Bayerische Akademie der Wissenschaften [6].\n",
    "\n",
    "Einf√ºhrung\n",
    "ECKHART ARNOLD (M√ºnchen), MARK HENGERER (M√ºnchen), NOAH BUBENHOFER (Z√ºrich) und CHRISTIAN RIEPL (M√ºnchen) machten klar, dass das Vorhandensein grosser Mengen an digitalen Textkorpora den Geisteswissenschaften neue Zukunftsperspektiven er√∂ffnet, gleichzeitig aber auch neue Herausforderungen an die Disziplinen stellt. Der Br√ºckenschlag zwischen Philologie und Informationstechnologie erfordert von den Forschenden ein hohes Mass an Technikaffinit√§t. Das Verst√§ndnis von Text als Daten und dessen interaktive graphische Visualisierung ver√§ndert die hermeneutischen Herangehensweisen und Methoden.\n",
    "\n",
    "Panel 1: Korpora\n",
    "Im Er√∂ffnungsreferat stellte PETER MAKAROV (Z√ºrich) sein PhD Projekt zum Thema ‚ÄûTowards automated content event analysis: Mining for protest events‚Äú vor. Es ist Teil des POLCON Projekts unter Professor Hanspeter Kriesi.[7] Das Projekt bewegt sich zwischen Politikwissenschaft und Computerlinguistik. Ziel ist, herauszufinden, inwiefern moderne Natural Language Processing Technik die thematische Extraktion von Daten aus dem Internet unterst√ºtzen kann. Konkret geht es in der Studie um die Extraktion von News-Texten im Zusammenhang mit √ñffentlichen Protesten.\n",
    "Methodisch setzt das Projekt auf Machine-Learning-Verfahren. Hierzu m√ºssen geeignete Klassifizierungsmodelle erstellt und Entwicklungszyklen implementiert werden, mit deren Hilfe Trainings-Korpora erstellt werden k√∂nnen. In einem iterativen Prozess werden diese Trainingsdaten annotiert und verbessert. Dabei hat sich gezeigt, dass linguistische Standardmodelle zur Annotation nicht geeignet waren, um die Identifikation von Textstellen, die Protest indizierten, zu verbessern. Daher werden vereinfachte, besser auf die konkrete Fragestellung zugeschnittene Annotationssysteme entwickelt.\n",
    "DANIEL KNUCHEL (Z√ºrich) referierte unter dem Titel ‚ÄûHIV/AIDS diskurslinguistisch ‚Äì ein multimediales Korpus‚Äú √ºber sein Promotionsprojekt. In diesem analysiert er, welches Wissen heute zum Thema HIV/AIDS in der √ñffentlichkeit zirkuliert. Dazu baute der Referent ein Korpus aus unterschiedlichen Quellen (Massenmedien, Selbsthilfe-Blogs, Social Media) auf. Er wies darauf hin, dass bei solchen Vorhaben nebst den konzeptuellen und technischen Fragen die rechtlichen Bedingungen zur Datennutzung fr√ºhzeitig gekl√§rt werden m√ºssen. Wichtig sei zudem, dass von Anfang an auf Nachhaltigkeit geachtet werde, um sp√§tere Nutzungen der Daten zu erm√∂glichen.\n",
    "MAX HADERSBECK (M√ºnchen) berichtete in seinem Vortrag √ºber die Erfahrungen mit der FinderApp ‚ÄúWittFind‚Äú.[8] Die webbasierte Applikation steht Forschenden nun seit vier Jahren zur Verf√ºgung, um den Open Access zug√§nglichen Teil des Nachlasses von Ludwig Wittgenstein nach W√∂rtern, Phrasen, S√§tzen und semantischen Begriffen zu durchsuchen. Sie setzt dazu regelbasierte computerlinguistische Verfahren ein. WittFind zeichnet sich dadurch aus, dass die gefundenen Textstellen zusammen mit dem Faksimileextrakt dargestellt werden und eine √úberpr√ºfung anhand des Originaltexts jederzeit m√∂glich ist.\n",
    "Unter anderem am Beispiel des englischen Neologismus ‚Äúcherpumple‚Äù [9] stellte SUSANNE GRANDMONTAGNE (M√ºnchen), das System ‚ÄûNeoCrawler‚Äú [10] vor. NeoCrawler verfolgt die Entstehung und Verbreitung von Neologismen im Internet. Die Ergebnisse werden automatisiert f√ºr weitere linguistische Analysen aufbereitet. Zudem stellt das System Zeitreihenverlaufsanalysen zur Verf√ºgung. Eine Benutzeroberfl√§che unterst√ºtzt die manuelle Kategorisierung der erhobenen Daten. Aus rechtlichen Gr√ºnden kann diese M√∂glichkeit jedoch nicht im Rahmen von Crowd Sourcing genutzt werden.\n",
    "Die prosopographische Erforschung der Herrschaftselite der Habsburgermonarchie steht im Zentrum des Projekts ‚ÄûKaiserhof‚Äú [11], das MARK HENGERER und GERHARD SCH√ñN (M√ºnchen) vorstellten. Nebst der eindeutigen Identifikation von Personen ist die Messbarkeit qualitativer Aspekte eine der haupts√§chlichen Herausforderungen, um Netzwerke und ‚ÄûReichweiten von Integration‚Äú visualisieren zu k√∂nnen. Dabei hat sich der Ansatz bew√§hrt, von Begriffen mittlerer Reichweite auszugehen. Visualisierungen (etwa von Verwandtschaftsbeziehungen oder Geolokalisierungen) sind in diesem Zusammenhang von hohem heuristischem Wert.\n",
    "\n",
    "Natural Language Processing / Suche\n",
    "Zeitangaben sind eine zentrale Information in historischen Dokumenten. Das war der Ausgangspunkt f√ºr die Pr√§sentation von NATALIA KORCHAGINA (Z√ºrich) zu ‚ÄúNatural language processing for historical documents‚Äú. Doch die maschinelle Textextraktion aus den oft handschriftlichen Dokumenten ist komplex. Ziel des Forschungsvorhabens der Referentin ist es, ein Tool f√ºr die automatisierte Extraktion von Zeitangaben aus historischen Texten zu entwickeln. Als Quellengrundlage dient ein Korpus von digitalisierten Schweizer Rechtstexten zwischen dem 10. und 18. Jahrhundert.\n",
    "In einer ersten Phase des Projekts wurde unter Nutzung des an der Universit√§t Heidelberg entwickelten Zeittaggers HeidelTime [12] ein fehlerfreies, aber kleines manuell annotiertes Gold Standard Korpus erstellt. Auf dieser Grundlage wird sodann mit einem hybriden Vorgehen, das machine-learning und regelbasierte Methoden kombiniert, ein gr√∂sseres Silver Standard Korpus erarbeitet, das f√ºr die Extraktion von Zeitangaben herangezogen werden kann.\n",
    "Zeitgen√∂ssische Rechtstexte standen im Zentrum des Forschungsprojekts von KYOKO SUGISAKI (Z√ºrich). Sie pr√§sentierte in ihrem  Vortrag unter dem Titel ‚ÄúNatural language processing in speziellen Textsorten, z.B. legislative Texte‚Äú ihre soeben abgeschlossene Doktorarbeit. Am Beispiel von online verf√ºgbaren Schweizer Gesetzestexten erstellte sie ein qualitativ hochwertiges Korpus von fachspezifischen Texten. Im Verlauf der Arbeiten zeigte sich, dass vorhandene Referenzkorpora (etwa die Sammlung Schweizerischer Rechtsquellen) genutzt werden k√∂nnen, jedoch an die Spezifika des Vorhabens angepasst werden m√ºssen. Mittels Kombination von verschiedenen Analyseverfahren und Tools (u.a. POS-tagging, morphosyntaktische Analyse, Style-Checking) konnte die Qualit√§t der Texterkennung deutlich verbessert werden.\n",
    "\n",
    "Visualisierung\n",
    "Unter dem Titel ‚ÄûVisualisierungen in den Digital Humanities‚Äú diskutierten NOAH BUBENHOFER, KLAUS ROTHENH√ÑUSLER und DANICA PAJOVIC (alle Z√ºrich) die theoretischen Grundlagen von Visualisierungen und hinterfragten g√§ngige Visualisierungspraktiken in den Digital Humanities. Ausgangspunkt ist die Feststellung, dass Visualisierungen nicht nur bei der Darstellung von Analyseergebnissen, sondern auch bei der Datenexploration eine wichtige Rolle spielen.\n",
    "Besonders bei explorativen Visualisierungen sind gem√§ss den  Referenten methodisch-technische Aspekte wichtig. Denn Diagramme sind nicht Bilder. Sie sind hoch abstrahierte Darstellungen, die Hypothesen √ºber Sachverhalte transportieren. Visualisierungen lassen sich entlang einer Reihe von grafischen, datentypischen, diagrammatischen, semiotischen, √§sthetischen, technischen und diskursiven Attributen kategorisieren und beurteilen.\n",
    "Anhand der Darstellung von Geokollokationen, das hei√üt von sprachlichen √Ñu√üerungen √ºber Orte zeigte Noah Bubenhofer, wie durch Sprechen eine Vorstellung von Welt konstruiert wird.[13] Die naheliegende Visualisierung von Geokollokationen auf einer Weltkarte ist dann ein voraussetzungsvoller Vorgang, der unhinterfragte Pr√§missen von Kartendarstellungen √ºbernimmt. Bubenhofer pl√§dierte daher daf√ºr, auch nicht kartenbasierte Visualisierungen in Betracht zu ziehen. Das Beispiel illustrierte, wie Denkstile,    Software und technische M√∂glichkeiten in Visualisierungen mit einfliessen und diese in gewisser Weise vorbestimmen.\n",
    "MATTHIAS REINERT (M√ºnchen) pr√§sentierte in seinem Referat ‚Äûdeutsche-biographie.de ‚Äì ein historisch biografisches Informationssystem. Computerlinguistischer Ansatz und Visualisierung‚Äú, das aus diesem Vorhaben resultierende Internetangebot.[14] In rund 48.000 Lexikonartikeln bietet es Informationen zu rund 540.000 Personen. F√ºr die zuverl√§ssige Volltexterfassung und -kodierung sowie den Normdatenabgleich von Personen und Orten wurden seit 2012 computerlinguistische Verfahren eingesetzt. Hierzu wurden lokale Grammatiken und eine korpora-spezifische Wortdatenbank erstellt. Das historisch-biografische Informationssystem erm√∂glicht eine Geo-Visualisierung und die Darstellung von Ego-Netzwerken. In der Diskussion betonte der Referent, dass im Zusammenhang mit einem solchen interaktiven Angebot eine transparente Kommunikation √ºber die M√∂glichkeiten und Grenzen des Systems und der Datenbasis unerl√§sslich ist, um den Nutzern die Einsch√§tzung der Evidenz der gewonnen Resultate zu erm√∂glichen.\n",
    "Unter dem Titel ‚ÄûTheatrescapes‚Äú [15] argumentierte TOBIAS ENGLMEIER (M√ºnchen), dass die stetig wachsenden, nun auch f√ºr die Geisteswissenschaften verf√ºgbaren Datenbest√§nde oft nur mit Techniken der Informationsvisualisierung erfasst werden k√∂nnten und interpretierbar seien. Das Projekt ‚ÄûTheatrescapes. Mapping Global Theatre Histories‚Äú nutzt f√ºr die interaktive Kartendarstellungen WebGL (Web Graphics Library) [16] und den Google Maps API [17]. Der Referent zeigte auf, dass mittels dieses pragmatischen Ansatzes die technischen H√ºrden bei der Georeferenzierung von grossen Datenbest√§nden mit vertretbarem Aufwand √ºberwunden und ansprechende Resultate wie etwa interaktive historisierte Kartendarstellungen erzielt werden k√∂nnen. Allerdings betonte er, wie schon Noah Bubenhofer vor ihm, dass Entscheidungen √ºber den Einsatz von bestimmter Software √ºber technische Aspekte hinausreichen und auch inhaltliche Konsequenzen haben.\n",
    "EMMA MAGES (M√ºnchen) stellte den ‚ÄûAudioatlas Siebenb√ºrgisch-S√§chsischer Dialekte‚Äú (ASD) vor, einen interaktiven Online-Atlas.[18] Er erschlie√üt eine umfangreiche Audiodokumentation deutscher Ortsdialekte Siebenb√ºrgens und der Marmarosch und macht diese in Transkription und Audioaufnahmen zug√§nglich. Nebst der eigentlichen Transkription wurde eine morphosyntaktische Etikettierung vorgenommen und eine Ontologie f√ºr die Erschliessung entworfen. Mittels Kartierung erlaubt der ASD unter anderem qualitative und quantitative Sichten auf die √∂rtliche Verteilung der Dialekte.\n",
    "In seinem Referat √ºber die Online-Plattform ‚ÄûVerbaAlpina‚Äú berichtete STEPHAN L√úCKE (M√ºnchen) √ºber die Herausforderungen, die sich bei diesem politische Grenzen √ºberschreitenden und mehrsprachigen Ansatz stellten.[19] Ziel des Langzeitprojekts ‚ÄûVerba Alpina‚Äú ist es, den sprachlich stark fragmentierten Alpenraum in seiner kultur- und sprachgeschichtlichen Zusammengeh√∂rigkeit zu erschliessen. Das Projekt fokussiert auf die Wechselbeziehung (sowohl in onomasiologischer wie semasiologischer Perspektive) zwischen W√∂rtern und bezeichneten Konzepten. Die sprachlichen Zusammenh√§nge werden erg√§nzt mit ethnographischen, historischen und politischen Aspekten und in einer interaktiven Online-Karte mit Crowd-Sourcing-Komponente dargestellt.\n",
    "\n",
    "Crowd\n",
    "Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing.[20] Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899.[21] Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte.\n",
    "Abschliessend stellte GERHARD SCH√ñN (M√ºnchen) das Projek ‚ÄûPlay4Science‚Äú [22] und die in diesem Rahmen entwickelte Spiel-Plattform ‚ÄûArtigo‚Äú vor. Das Projekt bringt Geisteswissenschaftler/innen, Informatiker/innen und Computerlinguist/innen zusammen, um zweckgerichtete soziale Spiel-Software (‚ÄûGames with a Purpose‚Äú (GWAP)) zu entwickeln, die seit einiger Zeit auch im wissenschaftlichen Bereich erfolgreich Crowd-Sourcing-Ans√§tze unterst√ºtzen. Ziel von Play4Science ist, eine anpassbare universelle Plattform anzubieten, die von allen F√§chern f√ºr verschiedenste Anwendungen sozialer Software genutzt werden kann.\n",
    "Die bereits realisierte Spiel-Plattform ‚ÄûArtigo‚Äú [23] l√§dt zur Verschlagwortung von Gem√§lden aus einer Bilddatenbank ein. Sie schaltet zwei Mitspieler zusammen, die unabh√§ngig voneinander relevant erscheinende Begriffe f√ºr dieselben Bilder eingeben. Die solcherart Peer-validierten Begriffe werden in der Datenbank gespeichert und sind f√ºr sp√§tere Suchabfragen nutzbar.\n",
    "\n",
    "Fazit\n",
    "Der Workshop bot einen guten √úberblick  √ºber den State-of-the-Art computerlinguistischer Ans√§tze f√ºr die digitale Aufbereitung von Textkorpora. Er wies auf die Herausforderungen hin, die sich bei der interdisziplin√§ren Zusammenarbeit an der Schnittstelle zwischen Technik und geisteswissenschaflticher Forschung stellen. Es wurde klar, dass eine fundierte Fragestellung entscheidend f√ºr den Erfolg von computerlinguistischen Vorhaben ist. Zugleich wurde aber unter dem Stichwort ‚Äûcode matters‚Äú auch betont, dass technologische Aspekte nicht vernachl√§ssigt werden d√ºrfen, da sie Einfluss auf Vorgehensweisen und Resultate haben. Von den Geisteswissenschaftlern muss daher verlangt werden, dass sie wissen, was die verwendeten Algorithmen tun. Dies gilt insbesondere auch f√ºr heuristisch und explorativ eingesetzte Visualisierungen, bei denen sich die Forschenden stets zu fragen haben, ob sie den generierten Visualisierungen genug kritisch gegen√ºberstehen. Unter geisteswissenschaftlichen Vorzeichen kann eine in den Visual Analytics mitunter unterstellte korrelationsbasierte ‚Äûground truth‚Äú nicht vorausgesetzt werden.\n",
    "In den Diskussionen hat sich ferner die Sicherstellung der Nachhaltigkeit in computerlinguistischen Vorhaben als zentraler Aspekt herausgestellt. Dabei geht es um mehr als Datenverf√ºgbarkeit. Entscheidend sind das Bewusstsein f√ºr die Br√ºchigkeit der Datengrundlagen und der Umgang mit Unsch√§rfen und Unvollkommenheiten. In einer weiteren Perspektive identifizierte der Workshop eine Reihe zentraler Erfolgsfaktoren f√ºr Digital Humanities-Projekte. So gilt es die rechtlichen Bedingungen f√ºr die Datennutzung fr√ºhzeitig zu kl√§ren, eine ausbauf√§hige Infrastruktur aufzubauen, die Mitarbeitenden auszuw√§hlen, auszubilden und zu begeistern sowie die langfristige Finanzierung sicherzustellen. Insgesamt bot die Tagung einen guten √úberblick √ºber die M√∂glichkeiten der maschinellen Analyse und Interpretation von Texten. Es herrschte Konsens dar√ºber, dass sich die Geisteswissenschaften dadurch in den kommenden Jahren stark ver√§ndern werden.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwenden der regul√§ren Ausdr√ºcke\n",
    "\n",
    "# alle Zahlen\n",
    "zahlen = re.findall(r'\\d+', example_text)\n",
    "\n",
    "# alle Anmerkungen, wie zum Beispiel \"[1]\"\n",
    "anmerkungen = re.findall(r'\\[\\d+\\]', example_text)\n",
    "\n",
    "# alle W√∂rter, die mit einem \"A\" oder \"a\" beginnen \n",
    "woerter_a = re.findall(r'\\b[aA]\\w*', example_text)\n",
    "\n",
    "# alle Bestandteile, die in Anf√ºhrungszeichen gesetzt sind\n",
    "in_anfuehrungszeichen = re.findall(r'\"([^\"]*)\"', example_text)\n",
    "\n",
    "# alle W√∂rter am Ende eines Abschnitts\n",
    "woerter_am_ende = re.findall(r'\\w+\\.$', example_text, re.MULTILINE)\n",
    "\n",
    "# Pr√ºfen Sie au√üerdem, ob es hier doppelte Leerzeichen gibt und wenn ja wie oft\n",
    "doppelte_leerzeichen_count = len(re.findall(r'  +', example_text))\n",
    "\n",
    "# Entfernen Sie alle Anmerkungen und √ºberfl√ºssigen Leerzeichen. Weisen Sie den Text einer neuen Variablen zu\n",
    "bereinigter_text = re.sub(r'\\[\\d+\\]', '', example_text)\n",
    "bereinigter_text = re.sub(r'\\s+', ' ', bereinigter_text).strip()\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"zahlen\": zahlen,\n",
    "    \"anmerkungen\": anmerkungen,\n",
    "    \"woerter_a\": woerter_a,\n",
    "    \"in_anfuehrungszeichen\": in_anfuehrungszeichen,\n",
    "    \"woerter_am_ende\": woerter_am_ende,\n",
    "    \"doppelte_leerzeichen_count\": doppelte_leerzeichen_count,\n",
    "    \"bereinigter_text\": bereinigter_text\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zahlen\n",
      "['1', '2016', '2', '14', '3', '4', '5', '6', '1', '7', '8', '9', '10', '11', '10', '18', '12', '13', '14', '48', '000', '540', '000', '2012', '15', '16', '17', '18', '19', '150', '20', '1864', '1899', '21', '4', '22', '4', '23']\n",
      "\n",
      "\n",
      "anmerkungen\n",
      "['[1]', '[2]', '[3]', '[4]', '[5]', '[6]', '[7]', '[8]', '[9]', '[10]', '[11]', '[12]', '[13]', '[14]', '[15]', '[16]', '[17]', '[18]', '[19]', '[20]', '[21]', '[22]', '[23]']\n",
      "\n",
      "\n",
      "woerter_a\n",
      "['Arbeitskreis', 'Analysen', 'aktuelle', 'Arbeitskreis', 'Akademie', 'ARNOLD', 'an', 'aber', 'auch', 'an', 'an', 'als', 'automated', 'analysis', 'aus', 'auf', 'annotiert', 'Annotation', 'auf', 'Annotationssysteme', 'AIDS', 'analysiert', 'AIDS', 'aus', 'auf', 'Anfang', 'an', 'auf', 'Applikation', 'Access', 'aus', 'anhand', 'anderem', 'am', 'automatisiert', 'Analysen', 'aufbereitet', 'Aus', 'Aspekte', 'Ansatz', 'auszugehen', 'Ausgangspunkt', 'aus', 'automatisierte', 'aus', 'Als', 'an', 'aber', 'annotiertes', 'Auf', 'abgeschlossene', 'Am', 'Arbeiten', 'an', 'angepasst', 'Analyseverfahren', 'a', 'Analyse', 'alle', 'Ausgangspunkt', 'Analyseergebnissen', 'auch', 'Aspekte', 'abstrahierte', 'Attributen', 'Anhand', 'auf', 'auch', 'Ansatz', 'aus', 'Angebot', 'argumentierte', 'auch', 'API', 'auf', 'Ansatzes', 'Aufwand', 'ansprechende', 'Allerdings', 'Aspekte', 'auch', 'Audioatlas', 'ASD', 'Atlas', 'Audiodokumentation', 'Audioaufnahmen', 'ASD', 'anderem', 'auf', 'Ansatz', 'Alpina', 'Alpenraum', 'auf', 'Aspekten', 'alpinistische', 'Alpenklubs', 'aufrechtzuerhalten', 'Arbeit', 'am', 'Abschliessend', 'Artigo', 'a', 'auch', 'Ans√§tze', 'anpassbare', 'anzubieten', 'allen', 'Anwendungen', 'Artigo', 'aus', 'Art', 'Ans√§tze', 'Aufbereitung', 'auf', 'an', 'aber', 'auch', 'Aspekte', 'auf', 'Algorithmen', 'auch', 'Analytics', 'als', 'Aspekt', 'als', 'ausbauf√§hige', 'aufzubauen', 'auszuw√§hlen', 'auszubilden', 'Analyse']\n",
      "\n",
      "\n",
      "in_anfuehrungszeichen\n",
      "[]\n",
      "\n",
      "\n",
      "woerter_am_ende\n",
      "['Textanalyse.', 'Methoden.', 'Protesten.', 'entwickelt.', 'erm√∂glichen.', 'ist.', 'werden.', 'Wert.', 'Jahrhundert.', 'kann.', 'werden.', 'spielen.', 'beurteilen.', 'vorbestimmen.', 'erm√∂glichen.', 'haben.', 'Dialekte.', 'dargestellt.', 'handelte.', 'kann.', 'nutzbar.', 'werden.', 'werden.']\n",
      "\n",
      "\n",
      "doppelte_leerzeichen_count\n",
      "4\n",
      "\n",
      "\n",
      "bereinigter_text\n",
      "Der Arbeitskreis ‚ÄûDigital Humanities Munich‚Äú (dhmuc) widmete seinen ersten Workshop 2016 dem Thema ‚ÄûDigitale Daten in den Geisteswissenschaften. Interdisziplin√§re Perspektiven f√ºr semantische und strukturelle Analysen‚Äú. Insgesamt 14 Vortr√§ge er√∂rterten aktuelle Forschungen und Infrastrukturen im Bereich der maschinellen Textanalyse. Folgende Institutionen zeichneten f√ºr die Organisation des Workshops verantwortlich: dhmuc ‚Äì Arbeitskreis ‚ÄúDigital Humanities Munich‚Äù, das Institut f√ºr Computerlinguistik Universit√§t Z√ºrich , das Historische Seminar der Ludwig-Maximilians-Universit√§t M√ºnchen , die IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universit√§t M√ºnchen sowie die Bayerische Akademie der Wissenschaften . Einf√ºhrung ECKHART ARNOLD (M√ºnchen), MARK HENGERER (M√ºnchen), NOAH BUBENHOFER (Z√ºrich) und CHRISTIAN RIEPL (M√ºnchen) machten klar, dass das Vorhandensein grosser Mengen an digitalen Textkorpora den Geisteswissenschaften neue Zukunftsperspektiven er√∂ffnet, gleichzeitig aber auch neue Herausforderungen an die Disziplinen stellt. Der Br√ºckenschlag zwischen Philologie und Informationstechnologie erfordert von den Forschenden ein hohes Mass an Technikaffinit√§t. Das Verst√§ndnis von Text als Daten und dessen interaktive graphische Visualisierung ver√§ndert die hermeneutischen Herangehensweisen und Methoden. Panel 1: Korpora Im Er√∂ffnungsreferat stellte PETER MAKAROV (Z√ºrich) sein PhD Projekt zum Thema ‚ÄûTowards automated content event analysis: Mining for protest events‚Äú vor. Es ist Teil des POLCON Projekts unter Professor Hanspeter Kriesi. Das Projekt bewegt sich zwischen Politikwissenschaft und Computerlinguistik. Ziel ist, herauszufinden, inwiefern moderne Natural Language Processing Technik die thematische Extraktion von Daten aus dem Internet unterst√ºtzen kann. Konkret geht es in der Studie um die Extraktion von News-Texten im Zusammenhang mit √ñffentlichen Protesten. Methodisch setzt das Projekt auf Machine-Learning-Verfahren. Hierzu m√ºssen geeignete Klassifizierungsmodelle erstellt und Entwicklungszyklen implementiert werden, mit deren Hilfe Trainings-Korpora erstellt werden k√∂nnen. In einem iterativen Prozess werden diese Trainingsdaten annotiert und verbessert. Dabei hat sich gezeigt, dass linguistische Standardmodelle zur Annotation nicht geeignet waren, um die Identifikation von Textstellen, die Protest indizierten, zu verbessern. Daher werden vereinfachte, besser auf die konkrete Fragestellung zugeschnittene Annotationssysteme entwickelt. DANIEL KNUCHEL (Z√ºrich) referierte unter dem Titel ‚ÄûHIV/AIDS diskurslinguistisch ‚Äì ein multimediales Korpus‚Äú √ºber sein Promotionsprojekt. In diesem analysiert er, welches Wissen heute zum Thema HIV/AIDS in der √ñffentlichkeit zirkuliert. Dazu baute der Referent ein Korpus aus unterschiedlichen Quellen (Massenmedien, Selbsthilfe-Blogs, Social Media) auf. Er wies darauf hin, dass bei solchen Vorhaben nebst den konzeptuellen und technischen Fragen die rechtlichen Bedingungen zur Datennutzung fr√ºhzeitig gekl√§rt werden m√ºssen. Wichtig sei zudem, dass von Anfang an auf Nachhaltigkeit geachtet werde, um sp√§tere Nutzungen der Daten zu erm√∂glichen. MAX HADERSBECK (M√ºnchen) berichtete in seinem Vortrag √ºber die Erfahrungen mit der FinderApp ‚ÄúWittFind‚Äú. Die webbasierte Applikation steht Forschenden nun seit vier Jahren zur Verf√ºgung, um den Open Access zug√§nglichen Teil des Nachlasses von Ludwig Wittgenstein nach W√∂rtern, Phrasen, S√§tzen und semantischen Begriffen zu durchsuchen. Sie setzt dazu regelbasierte computerlinguistische Verfahren ein. WittFind zeichnet sich dadurch aus, dass die gefundenen Textstellen zusammen mit dem Faksimileextrakt dargestellt werden und eine √úberpr√ºfung anhand des Originaltexts jederzeit m√∂glich ist. Unter anderem am Beispiel des englischen Neologismus ‚Äúcherpumple‚Äù stellte SUSANNE GRANDMONTAGNE (M√ºnchen), das System ‚ÄûNeoCrawler‚Äú vor. NeoCrawler verfolgt die Entstehung und Verbreitung von Neologismen im Internet. Die Ergebnisse werden automatisiert f√ºr weitere linguistische Analysen aufbereitet. Zudem stellt das System Zeitreihenverlaufsanalysen zur Verf√ºgung. Eine Benutzeroberfl√§che unterst√ºtzt die manuelle Kategorisierung der erhobenen Daten. Aus rechtlichen Gr√ºnden kann diese M√∂glichkeit jedoch nicht im Rahmen von Crowd Sourcing genutzt werden. Die prosopographische Erforschung der Herrschaftselite der Habsburgermonarchie steht im Zentrum des Projekts ‚ÄûKaiserhof‚Äú , das MARK HENGERER und GERHARD SCH√ñN (M√ºnchen) vorstellten. Nebst der eindeutigen Identifikation von Personen ist die Messbarkeit qualitativer Aspekte eine der haupts√§chlichen Herausforderungen, um Netzwerke und ‚ÄûReichweiten von Integration‚Äú visualisieren zu k√∂nnen. Dabei hat sich der Ansatz bew√§hrt, von Begriffen mittlerer Reichweite auszugehen. Visualisierungen (etwa von Verwandtschaftsbeziehungen oder Geolokalisierungen) sind in diesem Zusammenhang von hohem heuristischem Wert. Natural Language Processing / Suche Zeitangaben sind eine zentrale Information in historischen Dokumenten. Das war der Ausgangspunkt f√ºr die Pr√§sentation von NATALIA KORCHAGINA (Z√ºrich) zu ‚ÄúNatural language processing for historical documents‚Äú. Doch die maschinelle Textextraktion aus den oft handschriftlichen Dokumenten ist komplex. Ziel des Forschungsvorhabens der Referentin ist es, ein Tool f√ºr die automatisierte Extraktion von Zeitangaben aus historischen Texten zu entwickeln. Als Quellengrundlage dient ein Korpus von digitalisierten Schweizer Rechtstexten zwischen dem 10. und 18. Jahrhundert. In einer ersten Phase des Projekts wurde unter Nutzung des an der Universit√§t Heidelberg entwickelten Zeittaggers HeidelTime ein fehlerfreies, aber kleines manuell annotiertes Gold Standard Korpus erstellt. Auf dieser Grundlage wird sodann mit einem hybriden Vorgehen, das machine-learning und regelbasierte Methoden kombiniert, ein gr√∂sseres Silver Standard Korpus erarbeitet, das f√ºr die Extraktion von Zeitangaben herangezogen werden kann. Zeitgen√∂ssische Rechtstexte standen im Zentrum des Forschungsprojekts von KYOKO SUGISAKI (Z√ºrich). Sie pr√§sentierte in ihrem Vortrag unter dem Titel ‚ÄúNatural language processing in speziellen Textsorten, z.B. legislative Texte‚Äú ihre soeben abgeschlossene Doktorarbeit. Am Beispiel von online verf√ºgbaren Schweizer Gesetzestexten erstellte sie ein qualitativ hochwertiges Korpus von fachspezifischen Texten. Im Verlauf der Arbeiten zeigte sich, dass vorhandene Referenzkorpora (etwa die Sammlung Schweizerischer Rechtsquellen) genutzt werden k√∂nnen, jedoch an die Spezifika des Vorhabens angepasst werden m√ºssen. Mittels Kombination von verschiedenen Analyseverfahren und Tools (u.a. POS-tagging, morphosyntaktische Analyse, Style-Checking) konnte die Qualit√§t der Texterkennung deutlich verbessert werden. Visualisierung Unter dem Titel ‚ÄûVisualisierungen in den Digital Humanities‚Äú diskutierten NOAH BUBENHOFER, KLAUS ROTHENH√ÑUSLER und DANICA PAJOVIC (alle Z√ºrich) die theoretischen Grundlagen von Visualisierungen und hinterfragten g√§ngige Visualisierungspraktiken in den Digital Humanities. Ausgangspunkt ist die Feststellung, dass Visualisierungen nicht nur bei der Darstellung von Analyseergebnissen, sondern auch bei der Datenexploration eine wichtige Rolle spielen. Besonders bei explorativen Visualisierungen sind gem√§ss den Referenten methodisch-technische Aspekte wichtig. Denn Diagramme sind nicht Bilder. Sie sind hoch abstrahierte Darstellungen, die Hypothesen √ºber Sachverhalte transportieren. Visualisierungen lassen sich entlang einer Reihe von grafischen, datentypischen, diagrammatischen, semiotischen, √§sthetischen, technischen und diskursiven Attributen kategorisieren und beurteilen. Anhand der Darstellung von Geokollokationen, das hei√üt von sprachlichen √Ñu√üerungen √ºber Orte zeigte Noah Bubenhofer, wie durch Sprechen eine Vorstellung von Welt konstruiert wird. Die naheliegende Visualisierung von Geokollokationen auf einer Weltkarte ist dann ein voraussetzungsvoller Vorgang, der unhinterfragte Pr√§missen von Kartendarstellungen √ºbernimmt. Bubenhofer pl√§dierte daher daf√ºr, auch nicht kartenbasierte Visualisierungen in Betracht zu ziehen. Das Beispiel illustrierte, wie Denkstile, Software und technische M√∂glichkeiten in Visualisierungen mit einfliessen und diese in gewisser Weise vorbestimmen. MATTHIAS REINERT (M√ºnchen) pr√§sentierte in seinem Referat ‚Äûdeutsche-biographie.de ‚Äì ein historisch biografisches Informationssystem. Computerlinguistischer Ansatz und Visualisierung‚Äú, das aus diesem Vorhaben resultierende Internetangebot. In rund 48.000 Lexikonartikeln bietet es Informationen zu rund 540.000 Personen. F√ºr die zuverl√§ssige Volltexterfassung und -kodierung sowie den Normdatenabgleich von Personen und Orten wurden seit 2012 computerlinguistische Verfahren eingesetzt. Hierzu wurden lokale Grammatiken und eine korpora-spezifische Wortdatenbank erstellt. Das historisch-biografische Informationssystem erm√∂glicht eine Geo-Visualisierung und die Darstellung von Ego-Netzwerken. In der Diskussion betonte der Referent, dass im Zusammenhang mit einem solchen interaktiven Angebot eine transparente Kommunikation √ºber die M√∂glichkeiten und Grenzen des Systems und der Datenbasis unerl√§sslich ist, um den Nutzern die Einsch√§tzung der Evidenz der gewonnen Resultate zu erm√∂glichen. Unter dem Titel ‚ÄûTheatrescapes‚Äú argumentierte TOBIAS ENGLMEIER (M√ºnchen), dass die stetig wachsenden, nun auch f√ºr die Geisteswissenschaften verf√ºgbaren Datenbest√§nde oft nur mit Techniken der Informationsvisualisierung erfasst werden k√∂nnten und interpretierbar seien. Das Projekt ‚ÄûTheatrescapes. Mapping Global Theatre Histories‚Äú nutzt f√ºr die interaktive Kartendarstellungen WebGL (Web Graphics Library) und den Google Maps API . Der Referent zeigte auf, dass mittels dieses pragmatischen Ansatzes die technischen H√ºrden bei der Georeferenzierung von grossen Datenbest√§nden mit vertretbarem Aufwand √ºberwunden und ansprechende Resultate wie etwa interaktive historisierte Kartendarstellungen erzielt werden k√∂nnen. Allerdings betonte er, wie schon Noah Bubenhofer vor ihm, dass Entscheidungen √ºber den Einsatz von bestimmter Software √ºber technische Aspekte hinausreichen und auch inhaltliche Konsequenzen haben. EMMA MAGES (M√ºnchen) stellte den ‚ÄûAudioatlas Siebenb√ºrgisch-S√§chsischer Dialekte‚Äú (ASD) vor, einen interaktiven Online-Atlas. Er erschlie√üt eine umfangreiche Audiodokumentation deutscher Ortsdialekte Siebenb√ºrgens und der Marmarosch und macht diese in Transkription und Audioaufnahmen zug√§nglich. Nebst der eigentlichen Transkription wurde eine morphosyntaktische Etikettierung vorgenommen und eine Ontologie f√ºr die Erschliessung entworfen. Mittels Kartierung erlaubt der ASD unter anderem qualitative und quantitative Sichten auf die √∂rtliche Verteilung der Dialekte. In seinem Referat √ºber die Online-Plattform ‚ÄûVerbaAlpina‚Äú berichtete STEPHAN L√úCKE (M√ºnchen) √ºber die Herausforderungen, die sich bei diesem politische Grenzen √ºberschreitenden und mehrsprachigen Ansatz stellten. Ziel des Langzeitprojekts ‚ÄûVerba Alpina‚Äú ist es, den sprachlich stark fragmentierten Alpenraum in seiner kultur- und sprachgeschichtlichen Zusammengeh√∂rigkeit zu erschliessen. Das Projekt fokussiert auf die Wechselbeziehung (sowohl in onomasiologischer wie semasiologischer Perspektive) zwischen W√∂rtern und bezeichneten Konzepten. Die sprachlichen Zusammenh√§nge werden erg√§nzt mit ethnographischen, historischen und politischen Aspekten und in einer interaktiven Online-Karte mit Crowd-Sourcing-Komponente dargestellt. Crowd Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing. Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899. Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte. Abschliessend stellte GERHARD SCH√ñN (M√ºnchen) das Projek ‚ÄûPlay4Science‚Äú und die in diesem Rahmen entwickelte Spiel-Plattform ‚ÄûArtigo‚Äú vor. Das Projekt bringt Geisteswissenschaftler/innen, Informatiker/innen und Computerlinguist/innen zusammen, um zweckgerichtete soziale Spiel-Software (‚ÄûGames with a Purpose‚Äú (GWAP)) zu entwickeln, die seit einiger Zeit auch im wissenschaftlichen Bereich erfolgreich Crowd-Sourcing-Ans√§tze unterst√ºtzen. Ziel von Play4Science ist, eine anpassbare universelle Plattform anzubieten, die von allen F√§chern f√ºr verschiedenste Anwendungen sozialer Software genutzt werden kann. Die bereits realisierte Spiel-Plattform ‚ÄûArtigo‚Äú l√§dt zur Verschlagwortung von Gem√§lden aus einer Bilddatenbank ein. Sie schaltet zwei Mitspieler zusammen, die unabh√§ngig voneinander relevant erscheinende Begriffe f√ºr dieselben Bilder eingeben. Die solcherart Peer-validierten Begriffe werden in der Datenbank gespeichert und sind f√ºr sp√§tere Suchabfragen nutzbar. Fazit Der Workshop bot einen guten √úberblick √ºber den State-of-the-Art computerlinguistischer Ans√§tze f√ºr die digitale Aufbereitung von Textkorpora. Er wies auf die Herausforderungen hin, die sich bei der interdisziplin√§ren Zusammenarbeit an der Schnittstelle zwischen Technik und geisteswissenschaflticher Forschung stellen. Es wurde klar, dass eine fundierte Fragestellung entscheidend f√ºr den Erfolg von computerlinguistischen Vorhaben ist. Zugleich wurde aber unter dem Stichwort ‚Äûcode matters‚Äú auch betont, dass technologische Aspekte nicht vernachl√§ssigt werden d√ºrfen, da sie Einfluss auf Vorgehensweisen und Resultate haben. Von den Geisteswissenschaftlern muss daher verlangt werden, dass sie wissen, was die verwendeten Algorithmen tun. Dies gilt insbesondere auch f√ºr heuristisch und explorativ eingesetzte Visualisierungen, bei denen sich die Forschenden stets zu fragen haben, ob sie den generierten Visualisierungen genug kritisch gegen√ºberstehen. Unter geisteswissenschaftlichen Vorzeichen kann eine in den Visual Analytics mitunter unterstellte korrelationsbasierte ‚Äûground truth‚Äú nicht vorausgesetzt werden. In den Diskussionen hat sich ferner die Sicherstellung der Nachhaltigkeit in computerlinguistischen Vorhaben als zentraler Aspekt herausgestellt. Dabei geht es um mehr als Datenverf√ºgbarkeit. Entscheidend sind das Bewusstsein f√ºr die Br√ºchigkeit der Datengrundlagen und der Umgang mit Unsch√§rfen und Unvollkommenheiten. In einer weiteren Perspektive identifizierte der Workshop eine Reihe zentraler Erfolgsfaktoren f√ºr Digital Humanities-Projekte. So gilt es die rechtlichen Bedingungen f√ºr die Datennutzung fr√ºhzeitig zu kl√§ren, eine ausbauf√§hige Infrastruktur aufzubauen, die Mitarbeitenden auszuw√§hlen, auszubilden und zu begeistern sowie die langfristige Finanzierung sicherzustellen. Insgesamt bot die Tagung einen guten √úberblick √ºber die M√∂glichkeiten der maschinellen Analyse und Interpretation von Texten. Es herrschte Konsens dar√ºber, dass sich die Geisteswissenschaften dadurch in den kommenden Jahren stark ver√§ndern werden.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in results.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren Sie alle Personennamen (Vor- und Nachname) und Herkunftsorte.\n",
    "\n",
    "persona = re.findall(r'([A-Z√Ñ√ñ√ú]+ [A-Z√Ñ√ñ√ú]+) \\(([A-Z√Ñ√ñ√ú√üa-z√§√∂√º\\- ]+)\\)', example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ECKHART ARNOLD', 'M√ºnchen')\n",
      "('MARK HENGERER', 'M√ºnchen')\n",
      "('NOAH BUBENHOFER', 'Z√ºrich')\n",
      "('CHRISTIAN RIEPL', 'M√ºnchen')\n",
      "('PETER MAKAROV', 'Z√ºrich')\n",
      "('DANIEL KNUCHEL', 'Z√ºrich')\n",
      "('MAX HADERSBECK', 'M√ºnchen')\n",
      "('SUSANNE GRANDMONTAGNE', 'M√ºnchen')\n",
      "('GERHARD SCH√ñN', 'M√ºnchen')\n",
      "('NATALIA KORCHAGINA', 'Z√ºrich')\n",
      "('KYOKO SUGISAKI', 'Z√ºrich')\n",
      "('DANICA PAJOVIC', 'alle Z√ºrich')\n",
      "('MATTHIAS REINERT', 'M√ºnchen')\n",
      "('TOBIAS ENGLMEIER', 'M√ºnchen')\n",
      "('EMMA MAGES', 'M√ºnchen')\n",
      "('STEPHAN L√úCKE', 'M√ºnchen')\n",
      "('SIMON CLEMATIDE', 'Z√ºrich')\n",
      "('GERHARD SCH√ñN', 'M√ºnchen')\n"
     ]
    }
   ],
   "source": [
    "for p in persona:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
